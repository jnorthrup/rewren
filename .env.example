# Wren Coder Environment Configuration Example
# Copy this file to .wren/.env or .env and configure your API keys
# API keys are NEVER stored in files - only in environment variables
# See AUTHENTICATION.md for detailed configuration instructions

# ==============================================================================
# PRIMARY PROVIDERS
# ==============================================================================

# OpenAI - Most widely used provider
# Get API key: https://platform.openai.com/api-keys
# Models: GPT-4o, GPT-4-turbo, GPT-3.5-turbo, o1, o3
OPENAI_API_KEY="sk-..."
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional: only needed for custom endpoints

# Anthropic (Claude) - Strong reasoning and code generation
# Get API key: https://console.anthropic.com/
# Models: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
# Supports alias: CLAUDE_API_KEY
ANTHROPIC_API_KEY="sk-ant-..."
# CLAUDE_API_KEY="sk-ant-..."  # Alternative
ANTHROPIC_BASE_URL="https://api.anthropic.com/v1"  # Optional

# Google Gemini - Native multimodal, massive context (2M tokens)
# Get API key: https://aistudio.google.com/app/apikey
# Models: Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 2.0
GEMINI_API_KEY="AI..."

# Google Vertex AI - Enterprise Gemini access
# Get started: https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys
# Authentication: Application Default Credentials (gcloud auth) or API key
GOOGLE_CLOUD_PROJECT="your-project-id"
GOOGLE_CLOUD_LOCATION="us-central1"
GOOGLE_API_KEY="your-vertex-api-key"  # Alternative to ADC

# ==============================================================================
# OPENAI-COMPATIBLE PROVIDERS
# ==============================================================================

# NVIDIA NIM - 165+ models, ultra-fast inference, free tier
# Get API key: https://build.nvidia.com/
# Models: Llama, Nemotron, GPT-OSS, Mistral, Qwen, Gemma, and more
# Special: GPT-OSS models use Harmony format (multi-channel reasoning)
NVIDIA_API_KEY="nvapi-..."
NVIDIA_BASE_URL="https://integrate.api.nvidia.com/v1"  # Optional

# DeepSeek - Advanced reasoning, competitive pricing
# Get API key: https://platform.deepseek.com/
# Models: DeepSeek-V3, DeepSeek-R1 (reasoner), DeepSeek-Coder
DEEPSEEK_API_KEY="sk-..."
DEEPSEEK_BASE_URL="https://api.deepseek.com/v1"  # Optional

# OpenRouter - 400+ models, unified API for all providers
# Get API key: https://openrouter.ai/keys
# Models: Access to Anthropic, OpenAI, Google, Meta, xAI, and more
OPENROUTER_API_KEY="sk-or-v1-..."
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"  # Optional

# Groq - Fastest inference (500+ tok/sec), free tier
# Get API key: https://console.groq.com/
# Models: Llama 3.3, Mixtral 8x7B, Gemma 2 (ultra-fast on LPU)
GROQ_API_KEY="gsk_..."
GROQ_BASE_URL="https://api.groq.com/openai/v1"  # Optional

# Kilo Code - Cutting-edge model access
# Models: GPT-5, Claude 4, Gemini 2.5 Pro, Qwen3 Coder
KILO_API_KEY="..."
KILO_BASE_URL="https://oai.endpoints.kepler.ai.cloud.ovh.net/v1"  # Optional

# ==============================================================================
# REGIONAL PROVIDERS
# ==============================================================================

# Qwen (Alibaba Cloud DashScope) - Strong multilingual, code generation
# Get API key: https://dashscope.aliyuncs.com/
# Models: Qwen2.5-Coder, QwQ (reasoning), Qwen2.5
# Compliance: China data residency
# Supports alias: DASHSCOPE_API_KEY
QWEN_API_KEY="sk-..."
# DASHSCOPE_API_KEY="sk-..."  # Alternative (official DashScope name)
# Regional endpoints (optional)
# QWEN_BASE_URL="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"  # International
# QWEN_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"       # China

# Moonshot AI (Kimi) - Long context, Chinese optimization
# Get API key: https://platform.moonshot.cn/
# Models: Moonshot-v1 (8k/32k/128k), Kimi2
# Supports alias: KIMI_API_KEY
MOONSHOT_API_KEY="sk-..."
# KIMI_API_KEY="sk-..."  # Alternative
MOONSHOT_BASE_URL="https://api.moonshot.ai/v1"  # Optional

# Mistral AI - European provider, multilingual, code generation
# Get API key: https://console.mistral.ai/
# Models: Mistral Large, Mistral Medium, Codestral, Mixtral
MISTRAL_API_KEY="..."
MISTRAL_BASE_URL="https://api.mistral.ai/v1"  # Optional

# Cohere - RAG-optimized, enterprise features
# Get API key: https://dashboard.cohere.com/
# Models: Command R+, Command R, Command
COHERE_API_KEY="..."
COHERE_BASE_URL="https://api.cohere.ai/v1"  # Optional

# ==============================================================================
# ADDITIONAL PROVIDERS
# ==============================================================================

# xAI (Grok) - Real-time X (Twitter) data access
# Get API key: https://console.x.ai/
# Models: Grok-2, Grok-2-Vision, Grok-beta
# Supports alias: GROK_API_KEY
XAI_API_KEY="xai-..."
# GROK_API_KEY="xai-..."  # Alternative
XAI_BASE_URL="https://api.x.ai/v1"  # Optional

# Cerebras - World's fastest inference (1000+ tok/sec)
# Get API key: https://cloud.cerebras.ai/
# Models: Llama 3.1 8B, 70B (on wafer-scale engine)
CEREBRAS_API_KEY="csk-..."
CEREBRAS_BASE_URL="https://api.cerebras.ai/v1"  # Optional

# Hugging Face - Access to 1000+ open-source models
# Get API key: https://huggingface.co/settings/tokens
# Models: Thousands via serverless inference
HUGGINGFACE_API_KEY="hf_..."
HUGGINGFACE_BASE_URL="https://router.huggingface.co/v1"  # Optional

# Perplexity - Web-augmented responses with real-time search
# Get API key: https://www.perplexity.ai/settings/api
# Models: Sonar, Sonar Pro
PERPLEXITY_API_KEY="pplx-..."
PERPLEXITY_BASE_URL="https://api.perplexity.ai"  # Optional

# ==============================================================================
# SPECIAL PROVIDERS
# ==============================================================================

# Harmony (Azure OpenAI) - Multi-channel reasoning orchestrator
# Azure OpenAI with analysis/commentary/final channels
# Models: GPT-4o, GPT-4-turbo, GPT-3.5-turbo via Azure
HARMONY_API_KEY="your-azure-api-key"
HARMONY_BASE_URL="https://your-resource.openai.azure.com"

# VSCode LLM Provider - IDE integration
# Requires: Wren VSCode IDE Companion extension
# Models: GitHub Copilot and other VSCode models
VSCODE_LLM_PAT="your-pat-token"
WREN_IDE_SERVER_PORT="3000"  # Optional: default is 3000

# ==============================================================================
# DEBUGGING AND LOGGING
# ==============================================================================

# Chain of Thought (COT) Debugging
# Enable for Harmony format models (Azure, NVIDIA GPT-OSS)
# Shows analysis/commentary/final channels in stderr
WREN_DEBUG_COT="false"  # Set to "true" to enable
# WREN_COT_LOG_FILE="/path/to/cot.log"  # Future: write COT to file

# General debugging
DEBUG="0"  # Set to "1" to enable debug logging

# ==============================================================================
# REASONING MODEL CONFIGURATION
# ==============================================================================

# Default parameters for reasoning models (o1, o3, r1, DeepSeek-R1)
# These can be overridden per model in the provider tree
REASONING_EFFORT="high"              # Options: low, medium, high
REASONING_MAX_TOKENS="8192"          # Max tokens for reasoning output
INCLUDE_REASONING="true"             # Include reasoning in response
VERBOSITY="high"                     # Options: low, medium, high

# ==============================================================================
# CUSTOM ENDPOINTS
# ==============================================================================

# Azure OpenAI (custom endpoint)
# OPENAI_BASE_URL="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
# OPENAI_API_KEY="your-azure-key"

# Local Ollama (OpenAI-compatible)
# OPENAI_BASE_URL="http://localhost:11434/v1"
# OPENAI_API_KEY="ollama"  # Dummy key required

# Custom vLLM endpoint
# OPENAI_BASE_URL="http://your-server:8000/v1"
# OPENAI_API_KEY="your-token"

# ==============================================================================
# PROVIDER ENABLEMENT
# ==============================================================================

# Enable provider detail panels in UI (optional)
# Shows detailed provider information in the tree view
ENABLE_PROVIDER_PANELS="0"  # Set to "1" to enable

# ==============================================================================
# NOTES
# ==============================================================================

# 1. API keys are stored in environment variables only, never in files
# 2. Place this file in .wren/.env or .env (recommended: .wren/.env)
# 3. Wren searches for .env in this order:
#    - Current directory: .wren/.env, .env
#    - Parent directories (recursive)
#    - Home directory: ~/.wren/.env, ~/.env
# 4. Only the FIRST .env file found is loaded (no merging)
# 5. Add .env to .gitignore to avoid committing secrets
# 6. See AUTHENTICATION.md for detailed setup instructions
# 7. See MODEL_PROVIDERS.md for provider-specific configuration
# 8. Most BASE_URL variables are optional and use sensible defaults

# ==============================================================================
# SECURITY
# ==============================================================================

# - Never commit API keys to version control
# - Use project-specific keys for different environments
# - Rotate keys regularly
# - Set restrictive file permissions: chmod 600 .wren/.env
# - Monitor API usage through provider dashboards
# - Use environment-specific keys (dev/staging/prod)

# ==============================================================================
# GETTING STARTED
# ==============================================================================

# Minimal setup (choose one):
# 1. OpenAI only:
#    OPENAI_API_KEY="sk-..."
#
# 2. Free tier providers:
#    NVIDIA_API_KEY="nvapi-..."
#    GROQ_API_KEY="gsk_..."
#
# 3. Multi-provider (recommended):
#    OPENAI_API_KEY="sk-..."
#    ANTHROPIC_API_KEY="sk-ant-..."
#    NVIDIA_API_KEY="nvapi-..."
#
# For complete documentation, see:
# - AUTHENTICATION.md - Full authentication guide
# - MODEL_PROVIDERS.md - Provider details and model catalog
# - CLAUDE.md - Project architecture
